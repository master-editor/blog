{"id":9057,"date":"2022-09-26T22:16:11","date_gmt":"2022-09-26T16:46:11","guid":{"rendered":"https:\/\/www.middlewareinventory.com\/?p=9057"},"modified":"2022-09-26T22:21:20","modified_gmt":"2022-09-26T16:51:20","slug":"install-karpenter-on-existing-eks-cluster-migration-guide","status":"publish","type":"post","link":"https:\/\/www.middlewareinventory.com\/blog\/install-karpenter-on-existing-eks-cluster-migration-guide\/","title":{"rendered":"Install Karpenter on Existing EKS Cluster &#8211; Autoscaler Migration | Kubernetes"},"content":{"rendered":"<p>Karpenter Auto Scaler is fairly advanced and provides a lot of Customization options than its predecessor Cluster Auto Scaler. (CA)<\/p>\n<p><a href=\"https:\/\/www.middlewareinventory.com\/blog\/creating-eks-cluster-with-karpenter-auto-scaling-terraform-kubernetes\/\">In our previous article<\/a>, we have seen how to install and set up Karpenter Auto Scaler into a new EKS Cluster using Terraform.<\/p>\n<p>In this article, we are going to see how to install and configure the karpenter Auto scaler into an existing EKS Cluster.<\/p>\n<p>This can be also considered as a migration guide from Cluster Auto Scaler to Karpenter<\/p>\n<p><a href=\"https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/karpenter-ca-migraiton.jpg\"><img class=\"post-img alignnone wp-image-9076 size-full\" src=\"https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/karpenter-ca-migraiton.jpg\" alt=\"Karpenter EKS\" width=\"1748\" height=\"1240\" srcset=\"https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/karpenter-ca-migraiton.jpg 1748w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/karpenter-ca-migraiton-300x213.jpg 300w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/karpenter-ca-migraiton-1024x726.jpg 1024w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/karpenter-ca-migraiton-768x545.jpg 768w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/karpenter-ca-migraiton-1536x1090.jpg 1536w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/karpenter-ca-migraiton-400x284.jpg 400w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/karpenter-ca-migraiton-1080x766.jpg 1080w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/karpenter-ca-migraiton-90x65.jpg 90w\" sizes=\"(max-width: 1748px) 100vw, 1748px\" \/><\/a><\/p>\n<h2>Configuring Environment Variables<\/h2>\n<p>Open your favourite terminal and set the environment variables. I am using Mac OS and iterm2 as my terminal.<\/p>\n<p>You can choose a terminal as per your choice.<\/p>\n<p>Once the Environment variables are set in the terminal,\u00a0 you must continue to use the same terminal until the end of this post.<\/p>\n<p>If you are opening a new tab or new terminal, You might have to set these Environment variables once again before Proceeding<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\u21d2 export KARPENTER_VERSION=v0.16.1\r\n\u21d2 export CLUSTER_NAME=\"gritfy-01\"\r\n\u21d2 export AWS_DEFAULT_REGION=\"us-east-1\"\r\n\u21d2 export AWS_ACCOUNT_ID=\"$(aws sts get-caller-identity &#8211; query Account &#8211; output text)\"\r\n\u21d2 export CLUSTER_ENDPOINT=\"$(aws eks describe-cluster &#8211; name ${CLUSTER_NAME} &#8211; query \"cluster.endpoint\" &#8211; output text)\"\r\n\u21d2 echo $KARPENTER_VERSION $CLUSTER_NAME $AWS_DEFAULT_REGION $AWS_ACCOUNT_ID $CLUSTER_ENDPOINT\r\nv0.16.1 gritfy-01 us-east-1 75*********3 https:\/\/A271E5***************5AC8.gr7.us-east-1.eks.amazonaws.com<\/pre>\n<p>&nbsp;<\/p>\n<h2>Launch the Cloudformation template to Create IAM instance Role<\/h2>\n<p>Now we need to create the IAM instance Role which would be mapped for the EC2 worker nodes once the karpenter provisions them.<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\u21d2 curl -fsSL https:\/\/karpenter.sh\/\"${KARPENTER_VERSION}\"\/getting-started\/getting-started-with-eksctl\/cloudformation.yaml  &gt; $TEMPOUT \\\r\n&amp;&amp; aws cloudformation deploy \\\r\n  &#8211; stack-name \"Karpenter-${CLUSTER_NAME}\" \\\r\n  &#8211; template-file \"${TEMPOUT}\" \\\r\n  &#8211; capabilities CAPABILITY_NAMED_IAM \\\r\n  &#8211; parameter-overrides \"ClusterName=${CLUSTER_NAME}\"<\/pre>\n<p>&nbsp;<\/p>\n<h2>Creating IAM Identity Mapping<\/h2>\n<p>This command adds the Karpenter node role to your <code>aws-auth<\/code> config map, allowing nodes with this role to connect to the cluster.<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\u21d2 eksctl create iamidentitymapping \\\r\n  &#8211; username system:node:{{EC2PrivateDNSName}} \\\r\n  &#8211; cluster \"${CLUSTER_NAME}\" \\\r\n  &#8211; arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:role\/KarpenterNodeRole-${CLUSTER_NAME}\" \\\r\n  &#8211; group system:bootstrappers \\\r\n  &#8211; group system:nodes\r\n2022-09-05 14:06:51 [\u2139]  adding identity \"arn:aws:iam::751115992403:role\/KarpenterNodeRole-gritfy-01\" to auth ConfigMap<\/pre>\n<p>&nbsp;<\/p>\n<h2 id=\"create-the-karpentercontroller-iam-role\">Create the KarpenterController IAM Role<\/h2>\n<p>Karpenter requires permissions like launching instances. This will create an AWS IAM Role, and Kubernetes service account, and associate them using <a href=\"https:\/\/docs.aws.amazon.com\/emr\/latest\/EMR-on-EKS-DevelopmentGuide\/setting-up-enable-IAM.html\">IRSA<\/a>.<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\u21d2 eksctl create iamserviceaccount \\\r\n  &#8211; cluster \"${CLUSTER_NAME}\" &#8211; name karpenter &#8211; namespace karpenter \\\r\n  &#8211; role-name \"${CLUSTER_NAME}-karpenter\" \\\r\n  &#8211; attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy\/KarpenterControllerPolicy-${CLUSTER_NAME}\" \\\r\n  &#8211; role-only \\\r\n  &#8211; approve\r\n<\/pre>\n<p>If your cluster is not having an IRSA enabled. you might see some errors like this<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">2022-09-05 14:08:50 [!] no IAM OIDC provider associated with cluster, try 'eksctl utils associate-iam-oidc-provider &#8211; region=us-east-1 &#8211; cluster=gritfy-01'<\/pre>\n<p>in that case,\u00a0 you can allow the IRSA with the following command<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\u21d2 eksctl utils associate-iam-oidc-provider &#8211; region=us-east-1 &#8211; cluster=gritfy-01 &#8211; approve<\/pre>\n<p>now retry the previous command and it would succeed<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\u21d2 eksctl utils associate-iam-oidc-provider &#8211; region=us-east-1 &#8211; cluster=gritfy-01 &#8211; approve\r\n2022-09-05 14:09:34 [\u2139]  will create IAM Open ID Connect provider for cluster \"gritfy-01\" in \"us-east-1\"\r\n2022-09-05 14:09:36 [\u2714]  created IAM Open ID Connect provider for cluster \"gritfy-01\" in \"us-east-1\"\r\nKubernetes|\u21d2 eksctl create iamserviceaccount \\\r\n  &#8211; cluster \"${CLUSTER_NAME}\" &#8211; name karpenter &#8211; namespace karpenter \\\r\n  &#8211; role-name \"${CLUSTER_NAME}-karpenter\" \\\r\n  &#8211; attach-policy-arn \"arn:aws:iam::${AWS_ACCOUNT_ID}:policy\/KarpenterControllerPolicy-${CLUSTER_NAME}\" \\\r\n  &#8211; role-only \\\r\n  &#8211; approve\r\n2022-09-05 14:16:26 [\u2139]  1 iamserviceaccount (karpenter\/karpenter) was included (based on the include\/exclude rules)\r\n2022-09-05 14:16:26 [!]  serviceaccounts that exist in Kubernetes will be excluded, use &#8211; override-existing-serviceaccounts to override\r\n2022-09-05 14:16:26 [\u2139]  1 task: { create IAM role for serviceaccount \"karpenter\/karpenter\" }\r\n2022-09-05 14:16:26 [\u2139]  building iamserviceaccount stack \"eksctl-gritfy-01-addon-iamserviceaccount-karpenter-karpenter\"\r\n2022-09-05 14:16:27 [\u2139]  deploying stack \"eksctl-gritfy-01-addon-iamserviceaccount-karpenter-karpenter\"\r\n2022-09-05 14:16:27 [\u2139]  waiting for CloudFormation stack \"eksctl-gritfy-01-addon-iamserviceaccount-karpenter-karpenter\"\r\n2022-09-05 14:16:58 [\u2139]  waiting for CloudFormation stack \"eksctl-gritfy-01-addon-iamserviceaccount-karpenter-karpenter\"<\/pre>\n<p>&nbsp;<\/p>\n<h2 id=\"create-the-karpentercontroller-iam-role\">Install Karpenter using helm chart<\/h2>\n<p>this command would deploy the helm chart in your cluster.<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">helm upgrade &#8211; install &#8211; namespace karpenter &#8211; create-namespace \\\r\n  karpenter karpenter\/karpenter \\\r\n  &#8211; version ${KARPENTER_VERSION} \\\r\n  &#8211; set serviceAccount.annotations.\"eks\\.amazonaws\\.com\/role-arn\"=${KARPENTER_IAM_ROLE_ARN} \\\r\n  &#8211; set clusterName=${CLUSTER_NAME} \\\r\n  &#8211; set clusterEndpoint=${CLUSTER_ENDPOINT} \\\r\n  &#8211; set aws.defaultInstanceProfile=KarpenterNodeInstanceProfile-${CLUSTER_NAME} \\\r\n  &#8211; wait # for the defaulting webhook to install before creating a Provisioner<\/pre>\n<p>&nbsp;<\/p>\n<p>Once the helm chart is installed successfully you can validate if the <code>Karpenter<\/code> namespace is ready and it might have launched two pods<\/p>\n<p>You can validate it by issuing the following command<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\u21d2 kubectl get pods -n karpenter<\/pre>\n<p>If you use the following command you can see there are two containers in each pod<\/p>\n<p>Note*: This command uses <code>sort<\/code> and <code>column<\/code> Linux commands for formatting<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\u21d2 kubectl get pods -n karpenter  -o jsonpath='{\"PODNAME\\tNAMESPACE\\tCONTAINERS\"}{\"\\n\"}{range .items[*]}{\"\\n\"}{.metadata.name}{\"\\t\"}{.metadata.namespace}{\"\\t\"}{range .spec.containers[*]}{.name}{\"=&gt;\"}{.image}{\",\"}{end}{end}' |sort|column -t<\/pre>\n<p>If your PODS are up and running, it is a half victory for us<\/p>\n<p>Now we need to create a Provisioner.<\/p>\n<p>&nbsp;<\/p>\n<h2>What is provisioner in Karpenter?<\/h2>\n<p>Provisioner is the key differentiator between the legacy <strong>Cluster Auto Scaler<\/strong> and <strong>Karpenter<\/strong>.<\/p>\n<p>AWS Karpenter Provides a lot of customization possibilities for designing our worker nodes. Unlike the legacy Cluster Auto Scaler.<\/p>\n<p>In the provisioner definition, we can define what <strong>subnets<\/strong>, and what <strong>security groups<\/strong> to be used while launching the EC2 instances<\/p>\n<p>Before we cover some more on the provisioner part. Let us take a quick look on how a simple provisioner definition looks like<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"yaml\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">apiVersion: karpenter.sh\/v1alpha5\r\nkind: Provisioner\r\nmetadata:\r\n  name: default\r\nspec:\r\n  requirements:\r\n    - key: karpenter.sh\/capacity-type\r\n      operator: In\r\n      values: [\"spot\"]\r\n  limits:\r\n    resources:\r\n      cpu: 1000\r\n  provider:\r\n    subnetSelector:\r\n      karpenter.sh\/discovery: ${CLUSTER_NAME}\r\n    securityGroupSelector:\r\n      karpenter.sh\/discovery: ${CLUSTER_NAME}\r\n  ttlSecondsAfterEmpty: 30\r\nEOF<\/pre>\n<p>&nbsp;<\/p>\n<p>But we are not going to use this provisioner as it would create some default and minimal launch templates and worker nodes for us.<\/p>\n<p>So we are going to customize it further.<\/p>\n<p>As part of our customization, <strong>we need to create our own Launch template<\/strong><\/p>\n<p>Launch template, as the name suggest helps us to create Multiple EC2 instances with the same configuration ( template)<\/p>\n<p>&nbsp;<\/p>\n<h2>Why do we need a Customized Launch Template?<\/h2>\n<p>The provisioner can auto-create the launch templates with the default configuration but the problem is that<\/p>\n<p>Let&#8217;s say you want to customize the EC2 instances which are launched as worker nodes and perform some of the listed below tasks<\/p>\n<ul>\n<li>Change the Key pair (or) add a key pair with a new worker nodes<\/li>\n<li>Change the Disk Size or add more disks<\/li>\n<li>Customize the Subnet and\/or Security group<\/li>\n<li>Adding Special User startup data or scripts<\/li>\n<\/ul>\n<p>Except for local testing or for development purposes. I would encourage you to use your custom launch template<\/p>\n<p>Now let us get some necessary information of the cluster before we create the launch template.<\/p>\n<p>&nbsp;<\/p>\n<h2 id=\"create-the-karpentercontroller-iam-role\">Collect the Cluster Data &#8211; to add it to USER DATA of EC2<\/h2>\n<p>In this step, we are going to collect some configuration data from our EKS cluster.<\/p>\n<p>This data is needed for the Launch template user_data ( startup boot time script) configuration.<\/p>\n<p>user_data contains a list of shell commands\u00a0 ( shell script) that would be executed during the boot-up \/ start time of the worker node.<\/p>\n<p>&nbsp;<\/p>\n<h3><strong><span style=\"color: #003366;\">Why cluster-specific information is needed?\u00a0 for the Launch template<\/span><\/strong><\/h3>\n<p>We have mentioned this cluster-specific information that we are going to collect would be used in creating the proper startup (user_data)<\/p>\n<p>ideal EKS worker node startup script aka user_data would look something like this<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"shell\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">#!\/bin\/bash\r\nset -ex\r\nB64_CLUSTER_CA=LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeE1UQXdOekUwTXpneE5Gb1hEVE14TVRBd05URTBNemd4TkZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBS0Q4CmdoQ0JQM2ZaelhhMXgwL3lYa1RSQzlOb3F4cGtTYzFtanZiaFgra0ZmM0crUnEvM3FqNGpwS3hmbytoRjgrQzcKL3hnTjFRNjQ********anR0RW1YQVRSQTdYYkpuMmlnU1pGdTBhWkxiMnAvWjVHKzhkdXVxcm1WMHlHS2ZlQ21CbDNnZnVZCmREUEZCcUY4Z292bHZKc2hlRXA3OGEzSFI4TWx6SUUxaW10NVhzN24rMG9JU05QVWd4US8vZURkK2djOXhHcVYKdFJhZVJCcWJmNm9iajVGalZkbVpOSnowOTJsOVVYSjI3ZitEQUNuTmk0TUVVTWxEUUhFQ2dUY09RcC9ZcDZzQwpEV09wWFhVYW5GOEIwcm51U2MrUWlXVmJCdjZHS1FhUzR1SWFNWG5QYThCMnp4amJQQ0Z1L2YzZmtRT0pVMCtIClAyV2c1bThKclNNWmpWRFB3a2lKbERHTXFCdjZoZlJXbGNRYgotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\r\n\r\nAPI_SERVER_URL=https:\/\/A271E535C*******561C6445AC8.gr7.us-east-1.eks.amazonaws.com\r\n\r\nK8S_CLUSTER_DNS_IP=172.20.0.10\r\n\r\n\/etc\/eks\/bootstrap.sh gritfy-01 &#8211; b64-cluster-ca $B64_CLUSTER_CA &#8211; apiserver-endpoint $API_SERVER_URL &#8211; dns-cluster-ip $K8S_CLUSTER_DNS_IP<\/pre>\n<p>If you look at the last line of the preceding user_data shell script. you can see we are calling a specific script named <code>\/etc\/eks\/bootstrap.sh<\/code><\/p>\n<blockquote><p>This script <code>\/etc\/eks\/bootstrap.sh<\/code> would be available only if you choose the right AMI which can power your EKS cluster.<\/p>\n<p>You can find more about the <a href=\"https:\/\/docs.aws.amazon.com\/eks\/latest\/userguide\/eks-optimized-ami.html\">EKS optimized AMIs here<\/a><\/p><\/blockquote>\n<p>This script is to get the EC2 instance ready to be attached to the Kubernetes eco-system. this installs various tools like<\/p>\n<ul>\n<li>Docker\/ContinerD ( Container Runtime Engine)<\/li>\n<li>Kubelet<\/li>\n<li>Networking Plugins in CNI<\/li>\n<li>KubeProxy and more.<\/li>\n<\/ul>\n<p>&nbsp;<\/p>\n<p>If you look at the way we are invoking this script you can see we are passing three additional arguments such as<\/p>\n<ul>\n<li><code>b64-cluster-ca<\/code> Base 64 Cluster CA Certificate<\/li>\n<li><code>apiserver-endpoint<\/code> HTTPS URL of the API Server<\/li>\n<li><code>dns-cluster-ip<\/code> DNS IP of the Kubernetes ( hope you remember that K8s has its internal IP and DNS)<\/li>\n<\/ul>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\/etc\/eks\/bootstrap.sh gritfy-01 &#8211; b64-cluster-ca $B64_CLUSTER_CA &#8211; apiserver-endpoint $API_SERVER_URL &#8211; dns-cluster-ip $K8S_CLUSTER_DNS_IP<\/pre>\n<p>Now, where do we get this information?\u00a0 we can get this information using <code>aws eks<\/code> CLI command<\/p>\n<p>For this command to work you must have set up the aws CLI.<\/p>\n<p>If you are new to it, <a href=\"https:\/\/www.middlewareinventory.com\/blog\/aws-cli-ec2\/\">refer to our exclusive article on aws cli here<\/a><\/p>\n<p>Here are the commands you might have to use to get different key items<\/p>\n<p><strong>b64-cluster-ca<\/strong><\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\u21d2 aws eks describe-cluster &#8211; name ${CLUSTER_NAME} &#8211; query \"cluster.certificateAuthority.data\" &#8211; output text<\/pre>\n<p><strong>api-server-endpoint<\/strong><\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\u21d2 aws eks describe-cluster &#8211; name ${CLUSTER_NAME} &#8211; query \"cluster.endpoint\" &#8211; output text<\/pre>\n<p><strong>dns-cluster-ip<\/strong><\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\u21d2 aws eks describe-cluster &#8211; name ${CLUSTER_NAME} &#8211; query \"cluster.kubernetesNetworkConfig.serviceIpv4Cidr\" &#8211; output text<\/pre>\n<p>Once you have got the key items needed you can make a note of them and use them during the Launch template creation in the next step<\/p>\n<p>There is one more item you need to collect which is a security group associated with the existing nodes<\/p>\n<p>&nbsp;<\/p>\n<h3><span style=\"color: #003366;\">How to find the Security groups associated with my EKS cluster<\/span><\/h3>\n<p>You can find the master security group associated with your EKS cluster from the AWS Admin console itself<\/p>\n<p><span style=\"color: #003366;\">Go to EKS &gt; Clusters &gt; Choose the cluster<\/span><\/p>\n<p>In the Networking tab, you can find the primary security group and secondary security group.<\/p>\n<p>This Security group is not sufficient at times for the EC2 worker nodes.<\/p>\n<p>Since this is your existing EKS cluster there must be some additional configuration changes done to the node group and the Launch template underneath<\/p>\n<p>so the right way to check the security groups is to refer to the existing worker nodes<\/p>\n<p>you can do this by going here<\/p>\n<p><span style=\"color: #003366;\">Go to EKS &gt; Clusters &gt; Choose your cluster Compute &gt; Click on the existing worker node\u00a0<\/span><\/p>\n<p>Once the particular node is open click the <strong>instance_id\u00a0<\/strong> it would take you to the EC2 service screen where you can find the security groups associated with your existing worker node<\/p>\n<p>Once you have noted down the security groups. we can move on to creating the Launch template<\/p>\n<p>&nbsp;<\/p>\n<h2>Create a Launch template with Terraform<\/h2>\n<p>We have collected all the data needed now let us create the launch template.<\/p>\n<p>We are going to use <a href=\"https:\/\/registry.terraform.io\/providers\/hashicorp\/aws\/latest\/docs\/resources\/launch_template\">Terraform<\/a>\u00a0to create our launch template<\/p>\n<p>Terraform code for the Launch template creation is available on <a href=\"https:\/\/github.com\/AKSarav\/EKS_launch_template\">my github repository here<\/a><\/p>\n<p>You can clone the repository using the following command<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\u21d2 git clone https:\/\/github.com\/AKSarav\/EKS_launch_template<\/pre>\n<p>Once you have downloaded the code. You can start with <strong>terraform plan<\/strong><\/p>\n<p>You need to pass the necessary startup argument<\/p>\n<ul>\n<li><strong>security_group_ids\u00a0<\/strong>security group IDs for the launch template machines<\/li>\n<li><strong>ami_id\u00a0\u00a0<\/strong>amazon eks optimized ami<\/li>\n<li><strong>disksize <\/strong>EBS volume size of the worker nodes<\/li>\n<li><strong>lt_name\u00a0<\/strong>name to the launch template being created<\/li>\n<li><strong>iam_instance_profile\u00a0<\/strong>IAM instance profile name ( IAM role name)<\/li>\n<li><strong>keypair_name\u00a0<\/strong>Key Pair name, If do not have already, <a href=\"https:\/\/docs.aws.amazon.com\/AWSEC2\/latest\/UserGuide\/create-key-pairs.html\">create one<\/a><\/li>\n<\/ul>\n<p>&nbsp;<\/p>\n<p>here is the command, please make sure to update the values after the <code>=<\/code>sign before trying this at your end<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\u21d2 terraform plan -out tfplan.out \\\r\n  -var 'security_group_ids=[\"sg-xxxxxxxxxx\", \"sg-xxxxxxxxxx\"]' \\\r\n  -var ami_id=ami-0d78302dd24db83c7 \\\r\n  -var disksize=100 \\\r\n  -var lt_name=karpenter-lt-eks-qa01 \\\r\n  -var iam_instance_profile=KarpenterNodeInstanceProfile-gritfy-01 \\\r\n  -var keypair_name=jumphost-gritfy<\/pre>\n<p>&nbsp;<\/p>\n<p>we also have <code>-out<\/code> to store the output of the plan to <code>tfplan.out<\/code> file which would be used in the next stage<\/p>\n<p>Always make sure what changes are being done on the plan state so that you would not end up with surprises.<\/p>\n<p>Once you are satisfied with the plan.<\/p>\n<p>Now we can go ahead and execute <strong>terraform apply <\/strong>command to apply the changes.<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\u21d2 terraform apply tfplan.out<\/pre>\n<p>here is a glimpse of the execution output for both these commands<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">\u21d2 terraform plan -out tfplan.out -var ami_id=ami-0d78302dd24db83c7 -var disksize=100 -var lt_name=karpenter-lt-eks-qa01 -var iam_instance_profile=KarpenterNodeInstanceProfile-gritfy-01 -var keypair_name=jumphost-gritfy\r\n\r\nTerraform used the selected providers to generate the following execution plan. Resource actions are\r\nindicated with the following symbols:\r\n  + create\r\n\r\nTerraform will perform the following actions:\r\n\r\n  # aws_launch_template.eks-lt will be created\r\n  + resource \"aws_launch_template\" \"eks-lt\" {\r\n      + arn                    = (known after apply)\r\n      + default_version        = (known after apply)\r\n      + ebs_optimized          = \"true\"\r\n      + id                     = (known after apply)\r\n      + image_id               = \"ami-0d78302dd24db83c7\"\r\n      + key_name               = \"jumphost-pp\"\r\n      + latest_version         = (known after apply)\r\n      + name                   = \"karpenter-lt-eks-qa01\"\r\n      + name_prefix            = (known after apply)\r\n      + tags_all               = (known after apply)\r\n      + user_data              = \"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\"\r\n      + vpc_security_group_ids = [\r\n          + \"sg-xxxxxxxxxx\",\r\n          + \"sg-xxxxxxxxxx\",\r\n        ]\r\n\r\n      + block_device_mappings {\r\n          + device_name = \"\/dev\/sda1\"\r\n\r\n          + ebs {\r\n              + iops        = (known after apply)\r\n              + throughput  = (known after apply)\r\n              + volume_size = 100\r\n              + volume_type = (known after apply)\r\n            }\r\n        }\r\n\r\n      + iam_instance_profile {\r\n          + name = \"KarpenterNodeInstanceProfile-gritfy-01\"\r\n        }\r\n\r\n      + metadata_options {\r\n          + http_endpoint               = (known after apply)\r\n          + http_protocol_ipv6          = (known after apply)\r\n          + http_put_response_hop_limit = (known after apply)\r\n          + http_tokens                 = (known after apply)\r\n          + instance_metadata_tags      = (known after apply)\r\n        }\r\n\r\n      + monitoring {\r\n          + enabled = true\r\n        }\r\n    }\r\n\r\nPlan: 1 to add, 0 to change, 0 to destroy.\r\n\r\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\r\n\r\nSaved the plan to: tfplan.out\r\n\r\nTo perform exactly these actions, run the following command to apply:\r\n    terraform apply \"tfplan.out\"\r\nlaunchtemplate|\u21d2 terraform apply \"tfplan.out\"\r\naws_launch_template.eks-lt: Creating...\r\naws_launch_template.eks-lt: Creation complete after 2s [id=lt-0d56e9c204f8d5694]\r\n\r\nApply complete! Resources: 1 added, 0 changed, 0 destroyed.<\/pre>\n<p>&nbsp;<\/p>\n<p>Remember, we are going to use the launch template name <code>lt_name<\/code>\u00a0 within our provisioner.<\/p>\n<p>Now the launch template is ready. we can use this launch template in our provisioner.<\/p>\n<h2><\/h2>\n<h2>TAG the Subnets for the Provisioner<\/h2>\n<p>Now we have the launch template with the necessary information already configured like security_group ids and IAM roles etc.<\/p>\n<p>Now, we need to pass the subnet IDs to Karpenter<\/p>\n<h3><span style=\"color: #003366;\">How to identify the Subnets being used in my existing cluster<\/span><\/h3>\n<p>You can easily find the subnets of your existing cluster by checking the network settings of the currently running worker machine ( Since its a existing cluster)<\/p>\n<p>We can use the following AWS CLI commands to find out the subnets which are currently being used<\/p>\n<p>First List down the node groups of your existing EKS cluster with the following command<\/p>\n<pre># aws eks list-nodegroups &#8211; cluster-name dev-01<\/pre>\n<p>From the result of the previous command choose one of the returned node groups and describe it like below to get to the subnets<\/p>\n<pre>aws eks describe-nodegroup &#8211; cluster-name dev-01 &#8211; nodegroup-name &lt;nodegroup-name&gt; &#8211; query 'nodegroup.subnets' &#8211; output text<\/pre>\n<p>Here is the execution output of these commands, when executed at my end<\/p>\n<p><a href=\"https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-20-at-11.36.04-PM.png\"><img class=\"alignnone size-full wp-image-9068\" src=\"https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-20-at-11.36.04-PM.png\" alt=\"\" width=\"3096\" height=\"340\" srcset=\"https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-20-at-11.36.04-PM.png 3096w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-20-at-11.36.04-PM-300x33.png 300w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-20-at-11.36.04-PM-1024x112.png 1024w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-20-at-11.36.04-PM-768x84.png 768w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-20-at-11.36.04-PM-1536x169.png 1536w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-20-at-11.36.04-PM-2048x225.png 2048w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-20-at-11.36.04-PM-1080x119.png 1080w\" sizes=\"(max-width: 3096px) 100vw, 3096px\" \/><\/a><\/p>\n<p>You can tag these subnets manually or choose to tag either few of them based on the free IP availability<\/p>\n<p>Once you have decided on the subnets, you can use the aws cli command to tag them all<\/p>\n<pre class=\" language-bash\" tabindex=\"0\"><code class=\" language-bash\" data-lang=\"bash\">aws ec2 create-tags <span class=\"token punctuation\">\\<\/span>\r\n    &#8211; tags <span class=\"token string\">\"Key=karpenter.sh\/discovery,Value=<span class=\"token variable\">${CLUSTER_NAME}<\/span>\"<\/span> <span class=\"token punctuation\">\\<\/span>\r\n    &#8211; resources subnet-ids<\/code><\/pre>\n<p>Or you can manually tag them on the AWS Management console.<\/p>\n<pre>karpenter.sh\/discovery : cluster_name<\/pre>\n<p>Here is the snapshot taken at my end with a similar tag and my cluster name <code>gritfy-01<\/code><\/p>\n<p><a href=\"https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-18-at-12.32.32-AM-1.png\"><img class=\"alignnone size-full wp-image-9075\" src=\"https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-18-at-12.32.32-AM-1.png\" alt=\"\" width=\"1772\" height=\"466\" srcset=\"https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-18-at-12.32.32-AM-1.png 1772w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-18-at-12.32.32-AM-1-300x79.png 300w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-18-at-12.32.32-AM-1-1024x269.png 1024w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-18-at-12.32.32-AM-1-768x202.png 768w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-18-at-12.32.32-AM-1-1536x404.png 1536w, https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/Screenshot-2022-09-18-at-12.32.32-AM-1-1080x284.png 1080w\" sizes=\"(max-width: 1772px) 100vw, 1772px\" \/><\/a><\/p>\n<p>&nbsp;<\/p>\n<p>You can use the following one-liner shell command to find and tag all the existing subnets of your EKS cluster<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"generic\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">for NODEGROUP in $(aws eks list-nodegroups &#8211; cluster-name ${CLUSTER_NAME} \\\r\n    &#8211; query 'nodegroups' &#8211; output text); do aws ec2 create-tags \\\r\n        &#8211; tags \"Key=karpenter.sh\/discovery,Value=${CLUSTER_NAME}\" \\\r\n        &#8211; resources $(aws eks describe-nodegroup &#8211; cluster-name ${CLUSTER_NAME} \\\r\n        &#8211; nodegroup-name $NODEGROUP &#8211; query 'nodegroup.subnets' &#8211; output text )\r\ndone<\/pre>\n<p>&nbsp;<\/p>\n<h2>Quick Recap<\/h2>\n<ul>\n<li>We have set environment variables in our terminal with the cluster properties<\/li>\n<li>Created IAM instance role with Cloud Formation Template<\/li>\n<li>Created IAM Identity Mapping &#8211; Adding Entry to aws-auth config map<\/li>\n<li>Service Account creation with IAM role &#8211; <a href=\"https:\/\/docs.aws.amazon.com\/emr\/latest\/EMR-on-EKS-DevelopmentGuide\/setting-up-enable-IAM.html\">IRSA<\/a><\/li>\n<li>Installed Karpenter using Helm Chart<\/li>\n<li>Discussed what Karpenter Provisioner is and why we need a Custom Launch template<\/li>\n<li>Collected the Cluster config data and Security group for the Launch template Creation<\/li>\n<li>Created Launch template using Terraform and with the data collected in the previous step<\/li>\n<li>Identify the existing Subnets (or) create new ones and TAG them for the provisioner to find and use it<\/li>\n<\/ul>\n<p>These are the steps we have accomplished so far in our article and now we have everything in order for us to do our final step, creating the provisioner<\/p>\n<p>Unlike the Cluster Auto Scaler, We have a special controller named Karpenter provisioner which takes care of launching the nodes as per demand using our Custom Launch template and the Subnets<\/p>\n<p>with no further ado, let us create our provisioner<\/p>\n<p>&nbsp;<\/p>\n<h2>Creating the Provisioner<\/h2>\n<p>In the beginning, we have seen a default provisioner manifest file, It had minimal configuration required for the provisioner<\/p>\n<p>But we chose to use custom Launch templates as we had reasons.<\/p>\n<p>Now here is our modified provisioner YAML configuration file<\/p>\n<pre class=\"EnlighterJSRAW\" data-enlighter-language=\"yaml\" data-enlighter-theme=\"bootstrap4\" data-enlighter-linenumbers=\"false\">apiVersion: karpenter.sh\/v1alpha5\r\nkind: Provisioner\r\nmetadata:\r\n  name: default\r\nspec:\r\n  limits:\r\n    resources:\r\n      cpu: 2k\r\n  provider:\r\n    apiVersion: extensions.karpenter.sh\/v1alpha1\r\n    kind: AWS\r\n    launchTemplate: karpenter-lt-eks-qa01\r\n    subnetSelector:\r\n      karpenter.sh\/discovery: gritfy-01\r\n  requirements:\r\n  - key: karpenter.sh\/capacity-type\r\n    operator: In\r\n    values:\r\n    - spot\r\n  - key: kubernetes.io\/arch\r\n    operator: In\r\n    values:\r\n    - amd64\r\n  ttlSecondsAfterEmpty: 30<\/pre>\n<p>&nbsp;<\/p>\n<p>Let us decode this file to understand it better<\/p>\n<ul>\n<li>kind &#8211;\u00a0 to define what kind of definition it is, it is a provisioner<\/li>\n<li>metadata.name &#8211; what is the name of provisioner<\/li>\n<li>spec\n<ul>\n<li>limits.resources.cpu &#8211; maximum number of CPUs Karpenter can create ( you can also use memory in here)<\/li>\n<\/ul>\n<\/li>\n<li>provider\n<ul>\n<li>apiVersion &#8211; version of the karpeter definition<\/li>\n<li>kind &#8211; AWS provider ( Karpenter have future plans to extend to other cloud providers)<\/li>\n<li><strong>launchTemplate\u00a0<\/strong>&#8211; we are defining the name of the launch template we have created earlier in our article<\/li>\n<li><strong>subnetSelector &#8211;<\/strong> Since we have not hardcoded the subnet into the launch template and tagged them manually, we are defining what <code>tags<\/code> it should look for to find the right subnet.\u00a0 If you remember, we tagged the subnets earlier with <code>karpenter.sh\/discovery: cluster-name<\/code><\/li>\n<\/ul>\n<\/li>\n<li>requirements\n<ul>\n<li><strong>key: karpenter.sh\/capacity-type<\/strong> defining our requirements for the node, should it be spot or on-demand instance etc<\/li>\n<li><strong>key: kubernetes.io\/arch\u00a0<\/strong>defining what OS architecture that Karpenter can provision like <code>amd<\/code> or <code>arm<\/code> remember sometimes if you choose the architecture wrong or use both of them on the same cluster.\u00a0 Daemonsets and pods with specific OS architecture requirements might fail. you have to choose it right. Mostly it would be <code>amd<\/code><\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>&nbsp;<\/p>\n<blockquote><p>\u00a0Note*: Provisioner do provide various options to further customize your workload and requirements, <a href=\"https:\/\/karpenter.sh\/v0.16.2\/provisioner\/\">Please refer to this provisioner documentation<\/a> and write your own if you need to use the full potential of Karpenter<\/p><\/blockquote>\n<script async src=\"https:\/\/pagead2.googlesyndication.com\/pagead\/js\/adsbygoogle.js\"><\/script>\r\n<ins class=\"adsbygoogle\"\r\n     style=\"display:block; text-align:center;\"\r\n     data-ad-layout=\"in-article\"\r\n     data-ad-format=\"fluid\"\r\n     data-ad-client=\"ca-pub-3398911159151128\"\r\n     data-ad-slot=\"1946393371\"><\/ins>\r\n<script>\r\n     (adsbygoogle = window.adsbygoogle || []).push({});\r\n<\/script>\n<p>Now go ahead and apply this provisioner by saving this into some file named <code>yaml<\/code> or by copying the following <code>heredoc<\/code> shell command<\/p>\n<pre>$ cat &lt;&lt;EOF | kubectl apply -f -\r\napiVersion: karpenter.sh\/v1alpha5\r\nkind: Provisioner\r\nmetadata:\r\nname: default\r\nspec:\r\nlimits:\r\nresources:\r\ncpu: 2k\r\nprovider:\r\napiVersion: extensions.karpenter.sh\/v1alpha1\r\nkind: AWS\r\nlaunchTemplate: karpenter-lt-eks-qa01\r\nsubnetSelector:\r\nkarpenter.sh\/discovery: gritfy-01\r\nrequirements:\r\n- key: karpenter.sh\/capacity-type\r\noperator: In\r\nvalues:\r\n- spot\r\n- key: kubernetes.io\/arch\r\noperator: In\r\nvalues:\r\n- amd64\r\nttlSecondsAfterEmpty: 30\r\nEOF<\/pre>\n<p>Now once you have applied it either by yaml file or by the heredoc.<\/p>\n<p>You should see the Karpenter provisioner listed when you execute the following kubectl command<\/p>\n<pre>$ kubectl get provisioner default<\/pre>\n<p>Now your provisioner is ready<\/p>\n<p>Karpenter is now active and ready to begin provisioning nodes. Create some pods using a deployment, and watch Karpenter provision nodes in response.<\/p>\n<h2><\/h2>\n<h2>Validating Karpenter Logs<\/h2>\n<p>Now you must be able to see Karpenter pods running in a dedicated namespace called <code>karpenter<\/code><\/p>\n<p>Check if the pods are running and their names.<\/p>\n<pre>kubectl get pods -n karpenter<\/pre>\n<p>Once you know the karpenter POD names, check the logs<\/p>\n<pre>kubectl logs &lt;pod-name&gt; -n karpenter -c controller<\/pre>\n<p>Remember we spoke earlier that karpenter has a multi-container setup<\/p>\n<ul>\n<li>controller &#8211; which controls and manages the scheduling<\/li>\n<li>webhook &#8211; to keep the configuration in Sync with API server<\/li>\n<\/ul>\n<p>here in the preceding command, we are checking controller logs by mentioning <code>-c controller<\/code> in the kubectl logs command<\/p>\n<p>&nbsp;<\/p>\n<h2 id=\"remove-cas\">Remove Cluster Auto Scaler<\/h2>\n<p>Now that karpenter is running we can disable the cluster autoscaler. To do that we will scale the number of replicas to zero.<\/p>\n<div class=\"highlight\">\n<div class=\"code-toolbar\">\n<pre class=\" language-bash\" tabindex=\"0\"><code class=\" language-bash\" data-lang=\"bash\">kubectl scale deploy\/cluster-autoscaler -n kube-system &#8211; replicas<span class=\"token operator\">=<\/span><span class=\"token number\">0<\/span><\/code><\/pre>\n<\/div>\n<\/div>\n<p>To get rid of the instances that were added from the node group we can scale our nodegroup down to a minimum size to support Karpenter and other critical services. We suggest a minimum of 2 nodes for the node group.<\/p>\n<blockquote><p>Note: If your workloads do not have\u00a0<a href=\"https:\/\/kubernetes.io\/docs\/tasks\/run-application\/configure-pdb\/\">pod disruption budgets<\/a>\u00a0set, the following command\u00a0<strong>will cause workloads to be unavailable.<\/strong><\/p><\/blockquote>\n<div class=\"highlight\">\n<div class=\"code-toolbar\">\n<pre class=\" language-bash\" tabindex=\"0\"><code class=\" language-bash\" data-lang=\"bash\">aws eks update-nodegroup-config &#8211; cluster-name <span class=\"token variable\">${CLUSTER_NAME}<\/span> <span class=\"token punctuation\">\\<\/span>\r\n    &#8211; nodegroup-name <span class=\"token variable\">${NODEGROUP}<\/span> <span class=\"token punctuation\">\\<\/span>\r\n    &#8211; scaling-config <span class=\"token string\">\"minSize=2,maxSize=2,desiredSize=2\"<\/span><\/code><\/pre>\n<\/div>\n<\/div>\n<p>If you have a lot of nodes or workloads you may want to slowly scale down your node groups by a few instances at a time. It is recommended to watch the transition carefully for workloads that may not have enough replicas running or disruption budgets configured.<\/p>\n<p>&nbsp;<\/p>\n<h2 id=\"remove-cas\">Conclusion<\/h2>\n<p>In this lengthy article, I have tried to cover my experience and how did I implement Karpenter in our existing EKS cluster and migrated from Cluster Auto Scaler.<\/p>\n<p>If you have any questions or need additional clarifications anywhere, please do reach out to me in the comments. I will try to help.<\/p>\n<p>&nbsp;<\/p>\n<p>Cheers<br \/>\nRumen Lishkov<\/p>\n<div id=\"postfollow\">\r\n<a href='https:\/\/ko-fi.com\/O4O51FG7C' target='_blank'><img height='46' style='border:0px;height:46px;width: 186px' src='https:\/\/az743702.vo.msecnd.net\/cdn\/kofi3.png?v=2' border='0' alt='Buy Me a Coffee at ko-fi.com' \/><\/a>\r\n<p>Follow us on<a href=\"http:\/\/www.facebook.com\/middlewareinventory\">Facebook<\/a> or<a href=\"http:\/\/www.twitter.com\/mwinventory\">Twitter<\/a>\r\n<\/br>\r\nFor more practical videos and tutorials. <a href=\"https:\/\/www.youtube.com\/channel\/UCRuqBFM6ioWwviNJkgOjeWw?sub_confirmation=1\">Subscribe to our channel<\/a>\r\n<\/br>\r\nFollow me on Linkedin <a href=\"https:\/\/www.linkedin.com\/comm\/mynetwork\/discovery-see-all?usecase=PEOPLE_FOLLOWS&followMember=saravakmwinventory\">My Profile<\/a>\r\n<\/br>\r\n<\/i>For any Consultation or to hire us <a href=\"mailto:rumenlishkoff@gmail.com\"> rumenlishkoff@gmail.com<\/a>\r\n<\/br>\r\nIf you like this article. Show your Support! <a href=\"https:\/\/ko-fi.com\/middlewareinventory\">Buy me a Coffee.<\/a>\r\n<\/br>\r\n<\/p>\r\n<p style=\"color: palevioletred !important;\">Signup for Exclusive \"Subscriber-only\" Content<\/p>\r\n\n\t\t<div class=\"emaillist\">\n\t\t\t<form action=\"#\" method=\"post\" class=\"es_subscription_form es_shortcode_form\" id=\"es_subscription_form_1670577200\" data-source=\"ig-es\">\n\t\t\t\t\t\t\t\t<div class=\"es-field-wrap\"><label>Name*<br \/><input type=\"text\" name=\"name\" class=\"ig_es_form_field_name\" placeholder=\"\" value=\"\" required=\"required\" \/><\/label><\/div><div class=\"es-field-wrap\"><label>Email*<br \/><input class=\"es_required_field es_txt_email ig_es_form_field_email\" type=\"email\" name=\"email\" value=\"\" placeholder=\"\" required=\"required\" \/><\/label><\/div><input type=\"hidden\" name=\"lists[]\" value=\"1\" \/><input type=\"hidden\" name=\"form_id\" value=\"3\" \/>\n\t\t\t\t<input type=\"hidden\" name=\"es_email_page\" value=\"9057\"\/>\n\t\t\t\t<input type=\"hidden\" name=\"es_email_page_url\" value=\"https:\/\/www.middlewareinventory.com\/blog\/install-karpenter-on-existing-eks-cluster-migration-guide\/\"\/>\n\t\t\t\t<input type=\"hidden\" name=\"status\" value=\"Unconfirmed\"\/>\n\t\t\t\t<input type=\"hidden\" name=\"es-subscribe\" id=\"es-subscribe\" value=\"7aebfb1a9b\"\/>\n\t\t\t\t<label style=\"position:absolute;top:-99999px;left:-99999px;z-index:-99;\"><input type=\"email\" name=\"es_hp_email\" class=\"es_required_field\" tabindex=\"-1\" autocomplete=\"-1\" value=\"\"\/><\/label>\n\t\t\t\t\t\t\t\t<input type=\"submit\" name=\"submit\" class=\"es_subscription_form_submit es_submit_button es_textbox_button\" id=\"es_subscription_form_submit_1670577200\" value=\"Subscribe\"\/>\n\n\t\t\t\t\n\t\t\t\t<span class=\"es_spinner_image\" id=\"spinner-image\"><img src=\"https:\/\/www.middlewareinventory.com\/wp-content\/plugins\/email-subscribers\/lite\/public\/images\/spinner.gif\" alt=\"Loading\"\/><\/span>\n\n\t\t\t<\/form>\n\n\t\t\t<span class=\"es_subscription_message\" id=\"es_subscription_message_1670577200\"><\/span>\n\t\t<\/div>\n\n\t\t\r\n<\/div>\r\n<hr>\n","protected":false},"excerpt":{"rendered":"<p>Karpenter Auto Scaler is fairly advanced and provides a lot of Customization options than its predecessor Cluster Auto Scaler. (CA) In our previous article, we have seen how to install and set up Karpenter Auto Scaler into a new EKS Cluster using Terraform. In this article, we are going to[&#8230;]<\/p>\n","protected":false},"author":1,"featured_media":9076,"comment_status":"open","ping_status":"closed","sticky":false,"template":"","format":"standard","meta":{"_et_pb_use_builder":"","_et_pb_old_content":"","_et_gb_content_width":""},"categories":[234],"tags":[],"yoast_head":"<!-- This site is optimized with the Yoast SEO plugin v14.9 - https:\/\/yoast.com\/wordpress\/plugins\/seo\/ -->\n<title>Install Karpenter on Existing EKS Cluster - Autoscaler Migration | Kubernetes<\/title>\n<meta name=\"description\" content=\"Installing Karpenter on Existing EKS Cluster. Migrating from Cluster Autoscaler to Karpenter step by step. Karpenter EKS setup using Custom Launch template.\" \/>\n<meta name=\"robots\" content=\"index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1\" \/>\n<link rel=\"canonical\" href=\"https:\/\/www.middlewareinventory.com\/blog\/install-karpenter-on-existing-eks-cluster-migration-guide\/\" \/>\n<meta property=\"og:locale\" content=\"en_US\" \/>\n<meta property=\"og:type\" content=\"article\" \/>\n<meta property=\"og:title\" content=\"Install Karpenter on Existing EKS Cluster - Autoscaler Migration | Kubernetes\" \/>\n<meta property=\"og:description\" content=\"Installing Karpenter on Existing EKS Cluster. Migrating from Cluster Autoscaler to Karpenter step by step. Karpenter EKS setup using Custom Launch template.\" \/>\n<meta property=\"og:url\" content=\"https:\/\/www.middlewareinventory.com\/blog\/install-karpenter-on-existing-eks-cluster-migration-guide\/\" \/>\n<meta property=\"og:site_name\" content=\"Middleware Inventory\" \/>\n<meta property=\"article:publisher\" content=\"http:\/\/www.facebook.com\/devopsjunc\" \/>\n<meta property=\"article:published_time\" content=\"2022-09-26T16:46:11+00:00\" \/>\n<meta property=\"article:modified_time\" content=\"2022-09-26T16:51:20+00:00\" \/>\n<meta property=\"og:image\" content=\"https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/karpenter-ca-migraiton.jpg\" \/>\n\t<meta property=\"og:image:width\" content=\"1748\" \/>\n\t<meta property=\"og:image:height\" content=\"1240\" \/>\n<meta name=\"twitter:card\" content=\"summary_large_image\" \/>\n<meta name=\"twitter:creator\" content=\"@mwinventory\" \/>\n<meta name=\"twitter:site\" content=\"@mwinventory\" \/>\n<script type=\"application\/ld+json\" class=\"yoast-schema-graph\">{\"@context\":\"https:\/\/schema.org\",\"@graph\":[{\"@type\":\"Organization\",\"@id\":\"https:\/\/www.middlewareinventory.com\/#organization\",\"name\":\"Middleware Inventory\",\"url\":\"https:\/\/www.middlewareinventory.com\/\",\"sameAs\":[\"http:\/\/www.facebook.com\/devopsjunc\",\"https:\/\/www.youtube.com\/channel\/UCRuqBFM6ioWwviNJkgOjeWw\",\"https:\/\/twitter.com\/mwinventory\"],\"logo\":{\"@type\":\"ImageObject\",\"@id\":\"https:\/\/www.middlewareinventory.com\/#logo\",\"inLanguage\":\"en-US\",\"url\":\"https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2020\/09\/Screenshot-2020-09-18-at-3.49.40-AM.jpg\",\"width\":300,\"height\":107,\"caption\":\"Middleware Inventory\"},\"image\":{\"@id\":\"https:\/\/www.middlewareinventory.com\/#logo\"}},{\"@type\":\"WebSite\",\"@id\":\"https:\/\/www.middlewareinventory.com\/#website\",\"url\":\"https:\/\/www.middlewareinventory.com\/\",\"name\":\"Devops Junction\",\"description\":\"An inventory of [i]nformation, Middleware and much more.\",\"publisher\":{\"@id\":\"https:\/\/www.middlewareinventory.com\/#organization\"},\"potentialAction\":[{\"@type\":\"SearchAction\",\"target\":\"https:\/\/www.middlewareinventory.com\/?s={search_term_string}\",\"query-input\":\"required name=search_term_string\"}],\"inLanguage\":\"en-US\"},{\"@type\":\"ImageObject\",\"@id\":\"https:\/\/www.middlewareinventory.com\/blog\/install-karpenter-on-existing-eks-cluster-migration-guide\/#primaryimage\",\"inLanguage\":\"en-US\",\"url\":\"https:\/\/www.middlewareinventory.com\/wp-content\/uploads\/2022\/09\/karpenter-ca-migraiton.jpg\",\"width\":1748,\"height\":1240,\"caption\":\"Karpenter EKS\"},{\"@type\":\"WebPage\",\"@id\":\"https:\/\/www.middlewareinventory.com\/blog\/install-karpenter-on-existing-eks-cluster-migration-guide\/#webpage\",\"url\":\"https:\/\/www.middlewareinventory.com\/blog\/install-karpenter-on-existing-eks-cluster-migration-guide\/\",\"name\":\"Install Karpenter on Existing EKS Cluster - Autoscaler Migration | Kubernetes\",\"isPartOf\":{\"@id\":\"https:\/\/www.middlewareinventory.com\/#website\"},\"primaryImageOfPage\":{\"@id\":\"https:\/\/www.middlewareinventory.com\/blog\/install-karpenter-on-existing-eks-cluster-migration-guide\/#primaryimage\"},\"datePublished\":\"2022-09-26T16:46:11+00:00\",\"dateModified\":\"2022-09-26T16:51:20+00:00\",\"description\":\"Installing Karpenter on Existing EKS Cluster. Migrating from Cluster Autoscaler to Karpenter step by step. Karpenter EKS setup using Custom Launch template.\",\"inLanguage\":\"en-US\",\"potentialAction\":[{\"@type\":\"ReadAction\",\"target\":[\"https:\/\/www.middlewareinventory.com\/blog\/install-karpenter-on-existing-eks-cluster-migration-guide\/\"]}]},{\"@type\":\"Article\",\"@id\":\"https:\/\/www.middlewareinventory.com\/blog\/install-karpenter-on-existing-eks-cluster-migration-guide\/#article\",\"isPartOf\":{\"@id\":\"https:\/\/www.middlewareinventory.com\/blog\/install-karpenter-on-existing-eks-cluster-migration-guide\/#webpage\"},\"author\":{\"@id\":\"https:\/\/www.middlewareinventory.com\/#\/schema\/person\/050bf13e27b309d29d1bda45c4eb3147\"},\"headline\":\"Install Karpenter on Existing EKS Cluster &#8211; Autoscaler Migration | Kubernetes\",\"datePublished\":\"2022-09-26T16:46:11+00:00\",\"dateModified\":\"2022-09-26T16:51:20+00:00\",\"mainEntityOfPage\":{\"@id\":\"https:\/\/www.middlewareinventory.com\/blog\/install-karpenter-on-existing-eks-cluster-migration-guide\/#webpage\"},\"commentCount\":0,\"publisher\":{\"@id\":\"https:\/\/www.middlewareinventory.com\/#organization\"},\"image\":{\"@id\":\"https:\/\/www.middlewareinventory.com\/blog\/install-karpenter-on-existing-eks-cluster-migration-guide\/#primaryimage\"},\"articleSection\":\"AWS\",\"inLanguage\":\"en-US\",\"potentialAction\":[{\"@type\":\"CommentAction\",\"name\":\"Comment\",\"target\":[\"https:\/\/www.middlewareinventory.com\/blog\/install-karpenter-on-existing-eks-cluster-migration-guide\/#respond\"]}]},{\"@type\":\"Person\",\"@id\":\"https:\/\/www.middlewareinventory.com\/#\/schema\/person\/050bf13e27b309d29d1bda45c4eb3147\",\"name\":\"Rumen Lishkov\",\"image\":{\"@type\":\"ImageObject\",\"@id\":\"https:\/\/www.middlewareinventory.com\/#personlogo\",\"inLanguage\":\"en-US\",\"url\":\"https:\/\/secure.gravatar.com\/avatar\/c22e4a54d67750291a9892531f94ffb3?s=96&d=mm&r=g\",\"caption\":\"Rumen Lishkov\"}}]}<\/script>\n<!-- \/ Yoast SEO plugin. -->","_links":{"self":[{"href":"https:\/\/www.middlewareinventory.com\/wp-json\/wp\/v2\/posts\/9057"}],"collection":[{"href":"https:\/\/www.middlewareinventory.com\/wp-json\/wp\/v2\/posts"}],"about":[{"href":"https:\/\/www.middlewareinventory.com\/wp-json\/wp\/v2\/types\/post"}],"author":[{"embeddable":true,"href":"https:\/\/www.middlewareinventory.com\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/www.middlewareinventory.com\/wp-json\/wp\/v2\/comments?post=9057"}],"version-history":[{"count":6,"href":"https:\/\/www.middlewareinventory.com\/wp-json\/wp\/v2\/posts\/9057\/revisions"}],"predecessor-version":[{"id":9084,"href":"https:\/\/www.middlewareinventory.com\/wp-json\/wp\/v2\/posts\/9057\/revisions\/9084"}],"wp:featuredmedia":[{"embeddable":true,"href":"https:\/\/www.middlewareinventory.com\/wp-json\/wp\/v2\/media\/9076"}],"wp:attachment":[{"href":"https:\/\/www.middlewareinventory.com\/wp-json\/wp\/v2\/media?parent=9057"}],"wp:term":[{"taxonomy":"category","embeddable":true,"href":"https:\/\/www.middlewareinventory.com\/wp-json\/wp\/v2\/categories?post=9057"},{"taxonomy":"post_tag","embeddable":true,"href":"https:\/\/www.middlewareinventory.com\/wp-json\/wp\/v2\/tags?post=9057"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}